{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ebcd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import line_search\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21cf98c",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2a3f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rosenbrock function\n",
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Rosenbrock function.\n",
    "    f(x, y) = (a - x)^2 + b * (y - x^2)^2\n",
    "    Commonly, a=1 and b=100.\n",
    "    \"\"\"\n",
    "    a = 1.0\n",
    "    b = 100.0\n",
    "    return (a - x[0]) ** 2 + b * (x[1] - x[0] ** 2) ** 2\n",
    "\n",
    "\n",
    "# Define the gradient of the Rosenbrock function\n",
    "def rosenbrock_gradient(x):\n",
    "    \"\"\"\n",
    "    Gradient of the Rosenbrock function.\n",
    "    df/dx = -2*(a-x) - 2*b*(y-x^2)*2*x\n",
    "    df/dy = 2*b*(y-x^2)\n",
    "    \"\"\"\n",
    "    a = 1.0\n",
    "    b = 100.0\n",
    "    df_dx = -2 * (a - x[0]) - 4 * b * x[0] * (x[1] - x[0] ** 2)\n",
    "    df_dy = 2 * b * (x[1] - x[0] ** 2)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "\n",
    "# Define the Hessian of the Rosenbrock function\n",
    "def rosenbrock_hessian(x):\n",
    "    \"\"\"\n",
    "    Hessian of the Rosenbrock function.\n",
    "    d2f/dx2 = 2 - 4*b*(y-3*x^2)\n",
    "    d2f/dy2 = 2*b\n",
    "    d2f/dxdy = -4*b*x\n",
    "    d2f/dydx = -4*b*x\n",
    "    \"\"\"\n",
    "    a = 1.0\n",
    "    b = 100.0\n",
    "    h11 = 2 - 4 * b * (x[1] - 3 * x[0] ** 2)\n",
    "    h12 = -4 * b * x[0]\n",
    "    h21 = -4 * b * x[0]\n",
    "    h22 = 2 * b\n",
    "    return np.array([[h11, h12], [h21, h22]])\n",
    "\n",
    "\n",
    "# Golden Section Search for optimal step size\n",
    "def golden_section_search(func, x_k, p_k, a=0, b=1, tol=1e-5, max_iter=100):\n",
    "    \"\"\"\n",
    "    Golden Section Search for finding the optimal step size.\n",
    "    Minimizes func(x_k + alpha * p_k) with respect to alpha.\n",
    "\n",
    "    Args:\n",
    "        func: The objective function.\n",
    "        x_k: Current point.\n",
    "        p_k: Search direction.\n",
    "        a: Lower bound for alpha.\n",
    "        b: Upper bound for alpha.\n",
    "        tol: Tolerance for the search interval.\n",
    "        max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        The optimal step size alpha.\n",
    "    \"\"\"\n",
    "    phi = (1 + np.sqrt(5)) / 2  # Golden ratio\n",
    "    resphi = 2 - phi\n",
    "\n",
    "    x1 = a + resphi * (b - a)\n",
    "    x2 = b - resphi * (b - a)\n",
    "\n",
    "    f1 = func(x_k + x1 * p_k)\n",
    "    f2 = func(x_k + x2 * p_k)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        if abs(b - a) < tol:\n",
    "            return (a + b) / 2\n",
    "\n",
    "        if f1 < f2:\n",
    "            b = x2\n",
    "            x2 = x1\n",
    "            f2 = f1\n",
    "            x1 = a + resphi * (b - a)\n",
    "            f1 = func(x_k + x1 * p_k)\n",
    "        else:\n",
    "            a = x1\n",
    "            x1 = x2\n",
    "            f1 = f2\n",
    "            x2 = b - resphi * (b - a)\n",
    "            f2 = func(x_k + x2 * p_k)\n",
    "\n",
    "    return (a + b) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9bc5bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial approximation (random_state=42): [-0.50183952  1.80285723]\n",
      "Initial function value: 242.82006316689603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "\n",
    "initial_x = np.random.uniform(-2, 2, 2)\n",
    "print(f\"Initial approximation (random_state={random_state}): {initial_x}\")\n",
    "print(f\"Initial function value: {rosenbrock(initial_x)}\")\n",
    "\n",
    "FUNCTION_TOLERANCE = 10e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0875bc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Newton's Method ---\n",
      "Newton's Method Result: x=[0.99558475 0.99108911], f(x)=0.000020, Iterations: 15, Operations: {'func_evals': 16, 'grad_evals': 15, 'hess_evals': 15, 'inv_ops': 15}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Newton's Method Definition ---\n",
    "def newton_method(\n",
    "    func, grad, hess, x0, tol=FUNCTION_TOLERANCE, max_iter=1000, line_search_method=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Newton's Method for unconstrained optimization.\n",
    "\n",
    "    Args:\n",
    "        func: The objective function.\n",
    "        grad: The gradient of the objective function.\n",
    "        hess: The Hessian of the objective function.\n",
    "        x0: Initial guess.\n",
    "        tol: Tolerance for the function value.\n",
    "        max_iter: Maximum number of iterations.\n",
    "        line_search_method: Function for line search (e.g., golden_section_search).\n",
    "\n",
    "    Returns:\n",
    "        x_k: The found minimum point.\n",
    "        f_values: List of function values at each iteration.\n",
    "        iterations: Number of iterations performed.\n",
    "        operations: Dictionary counting function, gradient, Hessian evaluations, and inverse operations.\n",
    "    \"\"\"\n",
    "    x_k = np.array(x0, dtype=float)\n",
    "    f_values = [func(x_k)]\n",
    "    operations = {\"func_evals\": 1, \"grad_evals\": 0, \"hess_evals\": 0, \"inv_ops\": 0}\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        gradient = grad(x_k)\n",
    "        hessian = hess(x_k)\n",
    "        operations[\"grad_evals\"] += 1\n",
    "        operations[\"hess_evals\"] += 1\n",
    "\n",
    "        try:\n",
    "            hessian_inv = np.linalg.inv(hessian)\n",
    "            operations[\"inv_ops\"] += 1\n",
    "        except np.linalg.LinAlgError:\n",
    "            # print(f\"Newton's Method: Hessian is singular at iteration {i}. Stopping.\") # Removed print\n",
    "            break\n",
    "\n",
    "        p_k = -np.dot(hessian_inv, gradient)\n",
    "\n",
    "        alpha = 1.0\n",
    "\n",
    "        if line_search_method:\n",
    "            alpha = line_search_method(func, x_k, p_k)\n",
    "            if alpha is None:\n",
    "                # print(f\"Newton's Method: Line search failed at iteration {i}. Stopping.\") # Removed print\n",
    "                break\n",
    "\n",
    "        x_k = x_k + alpha * p_k\n",
    "        current_f_value = func(x_k)\n",
    "        f_values.append(current_f_value)\n",
    "        operations[\"func_evals\"] += 1\n",
    "\n",
    "        if current_f_value < tol:\n",
    "            # print(f\"Newton's Method converged to f(x) < {tol} at iteration {i+1}.\") # Removed print\n",
    "            break\n",
    "    else:\n",
    "        pass  # print(f\"Newton's Method reached max iterations ({max_iter}).\") # Removed print\n",
    "\n",
    "    return x_k, f_values, i + 1, operations\n",
    "\n",
    "\n",
    "# --- Newton's Method Usage ---\n",
    "print(\"\\n--- Running Newton's Method ---\")\n",
    "x_newton, f_values_newton, iter_newton, ops_newton = newton_method(\n",
    "    rosenbrock,\n",
    "    rosenbrock_gradient,\n",
    "    rosenbrock_hessian,\n",
    "    initial_x,\n",
    "    line_search_method=golden_section_search,\n",
    ")\n",
    "print(\n",
    "    f\"Newton's Method Result: x={x_newton}, f(x)={f_values_newton[-1]:.6f}, Iterations: {iter_newton}, Operations: {ops_newton}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99807b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Nesterov's Method ---\n",
      "Nesterov's Method Result: x=[-3.72865633 13.90910115], f(x)=22.364063, Iterations: 1000, Operations: {'func_evals': 1001, 'grad_evals': 1000}\n"
     ]
    }
   ],
   "source": [
    "# --- Nesterov's Accelerated Gradient Method Definition ---\n",
    "def nesterov_method(\n",
    "    func, grad, x0, tol=FUNCTION_TOLERANCE, max_iter=1000, learning_rate=0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Nesterov's Accelerated Gradient (NAG) Method.\n",
    "\n",
    "    Args:\n",
    "        func: The objective function.\n",
    "        grad: The gradient of the objective function.\n",
    "        x0: Initial guess.\n",
    "        tol: Tolerance for the function value.\n",
    "        max_iter: Maximum number of iterations.\n",
    "        learning_rate: Initial learning rate (alpha).\n",
    "\n",
    "    Returns:\n",
    "        x_k: The found minimum point.\n",
    "        f_values: List of function values at each iteration.\n",
    "        iterations: Number of iterations performed.\n",
    "        operations: Dictionary counting function and gradient evaluations.\n",
    "    \"\"\"\n",
    "    x_k = np.array(x0, dtype=float)\n",
    "    v_k = np.zeros_like(x0, dtype=float)  # Momentum term\n",
    "    f_values = [func(x_k)]\n",
    "    operations = {\"func_evals\": 1, \"grad_evals\": 0}\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        t_k = (i + 1) / 2  # Simple Nesterov's coefficient\n",
    "        y_k = x_k + (t_k / (t_k + 3)) * v_k  # Nesterov's acceleration\n",
    "\n",
    "        gradient = grad(y_k)\n",
    "        operations[\"grad_evals\"] += 1\n",
    "\n",
    "        # Check for NaN or inf in gradient\n",
    "        if np.isnan(gradient).any() or np.isinf(gradient).any():\n",
    "            print(\n",
    "                f\"Nesterov's Method: NaN or Inf encountered in gradient at iteration {i}. Stopping.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        v_k = v_k - learning_rate * gradient\n",
    "\n",
    "        # Check for NaN or inf in v_k\n",
    "        if np.isnan(v_k).any() or np.isinf(v_k).any():\n",
    "            print(\n",
    "                f\"Nesterov's Method: NaN or Inf encountered in momentum at iteration {i}. Stopping.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        x_k = x_k + v_k\n",
    "\n",
    "        current_f_value = func(x_k)\n",
    "        f_values.append(current_f_value)\n",
    "        operations[\"func_evals\"] += 1\n",
    "\n",
    "        # Check for NaN or inf in function value\n",
    "        if np.isnan(current_f_value) or np.isinf(current_f_value):\n",
    "            print(\n",
    "                f\"Nesterov's Method: NaN or Inf encountered in function value at iteration {i}. Stopping.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        if current_f_value < tol:\n",
    "            # print(f\"Nesterov's Method converged to f(x) < {tol} at iteration {i+1}.\") # Removed print\n",
    "            break\n",
    "    else:\n",
    "        pass  # print(f\"Nesterov's Method reached max iterations ({max_iter}).\") # Removed print\n",
    "\n",
    "    return x_k, f_values, i + 1, operations\n",
    "\n",
    "\n",
    "# --- Nesterov's Method Usage ---\n",
    "print(\"\\n--- Running Nesterov's Method ---\")\n",
    "# Adjusted learning rate for Rosenbrock to prevent overflow/NaN\n",
    "x_nesterov, f_values_nesterov, iter_nesterov, ops_nesterov = nesterov_method(\n",
    "    rosenbrock,\n",
    "    rosenbrock_gradient,\n",
    "    initial_x,\n",
    "    learning_rate=0.00005,  # Further reduced learning rate\n",
    ")\n",
    "print(\n",
    "    f\"Nesterov's Method Result: x={x_nesterov}, f(x)={f_values_nesterov[-1]:.6f}, Iterations: {iter_nesterov}, Operations: {ops_nesterov}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f726359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running SR1 Method ---\n",
      "SR1 Method Result: x=[0.11039602 0.0029492 ], f(x)=0.799929, Iterations: 1000, Operations: {'func_evals': 1001, 'grad_evals': 1001, 'inv_ops': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- SR1 (Symmetric Rank-1) Quasi-Newton Method Definition ---\n",
    "def sr1_method(\n",
    "    func, grad, x0, tol=FUNCTION_TOLERANCE, max_iter=1000, line_search_method=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Symmetric Rank-1 (SR1) Quasi-Newton Method.\n",
    "    Approximates the inverse Hessian.\n",
    "\n",
    "    Args:\n",
    "        func: The objective function.\n",
    "        grad: The gradient of the objective function.\n",
    "        x0: Initial guess.\n",
    "        tol: Tolerance for the function value.\n",
    "        max_iter: Maximum number of iterations.\n",
    "        line_search_method: Function for line search (e.g., golden_section_search).\n",
    "\n",
    "    Returns:\n",
    "        x_k: The found minimum point.\n",
    "        f_values: List of function values at each iteration.\n",
    "        iterations: Number of iterations performed.\n",
    "        operations: Dictionary counting function and gradient evaluations.\n",
    "    \"\"\"\n",
    "    x_k = np.array(x0, dtype=float)\n",
    "    n = len(x0)\n",
    "    H_k = np.eye(n)  # Initial approximation of the inverse Hessian\n",
    "\n",
    "    f_values = [func(x_k)]\n",
    "    operations = {\n",
    "        \"func_evals\": 1,\n",
    "        \"grad_evals\": 0,\n",
    "        \"inv_ops\": 0,\n",
    "    }  # No hess_evals for quasi-Newton\n",
    "\n",
    "    prev_grad = grad(x_k)\n",
    "    operations[\"grad_evals\"] += 1\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        p_k = -np.dot(H_k, prev_grad)\n",
    "\n",
    "        alpha = 1.0\n",
    "\n",
    "        if line_search_method:\n",
    "            alpha = line_search_method(func, x_k, p_k)\n",
    "            if alpha is None:\n",
    "                # print(f\"SR1 Method: Line search failed at iteration {i}. Stopping.\") # Removed print\n",
    "                break\n",
    "\n",
    "        s_k = alpha * p_k  # Step taken\n",
    "        x_k_plus_1 = x_k + s_k\n",
    "\n",
    "        current_grad = grad(x_k_plus_1)\n",
    "        operations[\"grad_evals\"] += 1\n",
    "\n",
    "        y_k = current_grad - prev_grad  # Change in gradient\n",
    "\n",
    "        Hy = np.dot(H_k, y_k)\n",
    "        denominator = np.dot(y_k, s_k) - np.dot(y_k, Hy)\n",
    "\n",
    "        if (\n",
    "            abs(denominator) > 1e-8\n",
    "        ):  # A small threshold to prevent numerical instability\n",
    "            term = s_k - Hy\n",
    "            H_k = H_k + np.outer(term, term) / denominator\n",
    "        else:\n",
    "            # print(f\"SR1 Method: Skipping update due to small denominator at iteration {i}.\") # Removed print\n",
    "            pass\n",
    "\n",
    "        x_k = x_k_plus_1\n",
    "        prev_grad = current_grad\n",
    "\n",
    "        current_f_value = func(x_k)\n",
    "        f_values.append(current_f_value)\n",
    "        operations[\"func_evals\"] += 1\n",
    "\n",
    "        if current_f_value < tol:\n",
    "            # print(f\"SR1 Method converged to f(x) < {tol} at iteration {i+1}.\") # Removed print\n",
    "            break\n",
    "    else:\n",
    "        pass  # print(f\"SR1 Method reached max iterations ({max_iter}).\") # Removed print\n",
    "\n",
    "    return x_k, f_values, i + 1, operations\n",
    "\n",
    "# --- SR1 Method Usage ---\n",
    "print(\"\\n--- Running SR1 Method ---\")\n",
    "x_sr1, f_values_sr1, iter_sr1, ops_sr1 = sr1_method(\n",
    "    rosenbrock, rosenbrock_gradient, initial_x, line_search_method=golden_section_search\n",
    ")\n",
    "print(\n",
    "    f\"SR1 Method Result: x={x_sr1}, f(x)={f_values_sr1[-1]:.6f}, Iterations: {iter_sr1}, Operations: {ops_sr1}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylab1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
